{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ab2cf0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T09:12:12.670948Z",
     "iopub.status.busy": "2025-11-24T09:12:12.670742Z",
     "iopub.status.idle": "2025-11-24T09:12:12.676987Z",
     "shell.execute_reply": "2025-11-24T09:12:12.676372Z"
    },
    "papermill": {
     "duration": 0.010448,
     "end_time": "2025-11-24T09:12:12.677998",
     "exception": false,
     "start_time": "2025-11-24T09:12:12.667550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tesy\n"
     ]
    }
   ],
   "source": [
    "print(\"tesy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1702c356",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T09:12:12.683873Z",
     "iopub.status.busy": "2025-11-24T09:12:12.683697Z",
     "iopub.status.idle": "2025-11-24T09:12:25.320237Z",
     "shell.execute_reply": "2025-11-24T09:12:25.319310Z"
    },
    "papermill": {
     "duration": 12.641142,
     "end_time": "2025-11-24T09:12:25.321406",
     "exception": false,
     "start_time": "2025-11-24T09:12:12.680264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting A-JEPA with 1D CNN implementation for Environment Sound Classification\n",
      "CUDA cache cleared.\n",
      "Error: Dataset path /kaggle/input/environment-sound/Environment_Sound does not exist.\n",
      "Available paths:\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import Subset\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from einops import rearrange\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Add at the end of each epoch in both training functions\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Environment Sound Dataset classes (will be populated from the dataset directory)\n",
    "ENV_SOUND_CLASSES = []\n",
    "\n",
    "# Audio processing parameters\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION = 10  # seconds\n",
    "SEQUENCE_LENGTH = SAMPLE_RATE * DURATION  # 160,000 samples for 10 seconds\n",
    "\n",
    "# Model parameters\n",
    "LATENT_DIM = 384  # Embedding dimension\n",
    "FEATURE_DIM = 512  # Feature dimension for CNN frontend\n",
    "NUM_LAYERS = 6     # Encoder layers\n",
    "NUM_DECODER_LAYERS = 8  # Decoder layers\n",
    "NUM_HEADS = 8     # Attention heads\n",
    "MLP_RATIO = 4     # Ratio for MLP hidden dim\n",
    "NUM_CLASSES = len(ENV_SOUND_CLASSES)  # Number of classes for classification\n",
    "CNN_KERNEL_SIZE = 400   # Kernel size for CNN front-end (25ms at 16kHz)\n",
    "CNN_STRIDE = 160        # Stride for CNN front-end (10ms at 16kHz)\n",
    "NUM_TOKENS = SEQUENCE_LENGTH // CNN_STRIDE  # Number of tokens after CNN\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS_PRETRAIN = 24\n",
    "NUM_EPOCHS_FINETUNE = 60\n",
    "LEARNING_RATE_PRETRAIN = 2e-4\n",
    "LEARNING_RATE_FINETUNE = 5e-5\n",
    "WEIGHT_DECAY = 0.05\n",
    "MASK_RATIO_FINETUNE = 0.1  # 10% regularized patch masking for fine-tuning\n",
    "\n",
    "# Path to dataset\n",
    "DATA_PATH = \"/kaggle/input/environment-sound/Environment_Sound\"  # Correct path to Environment Sound dataset\n",
    "\n",
    "# Define dataset class for raw audio\n",
    "class EnvironmentSoundDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Dataset root directory with class-named subfolders.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.classes = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "\n",
    "        self.audio_files = []\n",
    "        self.labels = []\n",
    "\n",
    "        for class_name in self.classes:\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            for fname in os.listdir(class_path):\n",
    "                if fname.endswith(\".wav\"):\n",
    "                    self.audio_files.append(os.path.join(class_path, fname))\n",
    "                    self.labels.append(self.class_to_idx[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_files[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Load audio\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "        # Convert to mono\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # Resample if needed\n",
    "        if sample_rate != SAMPLE_RATE:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=SAMPLE_RATE)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        # Pad/crop to fixed duration\n",
    "        target_length = SAMPLE_RATE * DURATION\n",
    "        current_length = waveform.shape[1]\n",
    "        if current_length < target_length:\n",
    "            padding = target_length - current_length\n",
    "            waveform = F.pad(waveform, (0, padding))\n",
    "        elif current_length > target_length:\n",
    "            start = np.random.randint(0, current_length - target_length)\n",
    "            waveform = waveform[:, start:start + target_length]\n",
    "\n",
    "        # Normalize waveform\n",
    "        waveform = (waveform - waveform.mean()) / (waveform.std() + 1e-6)\n",
    "\n",
    "        return waveform, label\n",
    "\n",
    "# 1D CNN Feature Extractor (similar to wav2Vec approach)\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=FEATURE_DIM, kernel_size=CNN_KERNEL_SIZE, \n",
    "                 stride=CNN_STRIDE, embed_dim=LATENT_DIM):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial conv layer to handle the raw audio - same as wav2vec approach\n",
    "        self.initial_conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride, padding=kernel_size//2),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        \n",
    "        # Multi-scale 1D CNN feature extractor\n",
    "        self.conv_blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.GELU(),\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.GELU()\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.GELU(),\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.GELU()\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.GELU(),\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.GELU()\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        # Projection to embedding dimension\n",
    "        self.projection = nn.Linear(out_channels, embed_dim)\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Calculate number of tokens after convolution (with padding)\n",
    "        self.num_tokens = (SEQUENCE_LENGTH + 2*(kernel_size//2) - kernel_size) // stride + 1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input: (B, C, L) - [batch, channels, length]\n",
    "        \n",
    "        # Apply initial convolution\n",
    "        x = self.initial_conv(x)\n",
    "        \n",
    "        # Apply additional CNN blocks\n",
    "        for conv_block in self.conv_blocks:\n",
    "            x = conv_block(x)\n",
    "        \n",
    "        # Transpose: (B, C, L) -> (B, L, C)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Project to embedding dimension\n",
    "        x = self.projection(x)\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        # Output: (B, L, D) - [batch, sequence_length, embed_dim]\n",
    "        return x\n",
    "\n",
    "# Define the MLP block\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features * MLP_RATIO\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "# Define the transformer encoder block\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., drop=0., attn_drop=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, dropout=attn_drop, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(in_features=dim, hidden_features=int(dim * mlp_ratio), drop=drop)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        if mask is not None:\n",
    "            # Apply mask to attention if provided\n",
    "            attn_output, _ = self.attn(self.norm1(x), self.norm1(x), self.norm1(x), \n",
    "                                     key_padding_mask=mask)\n",
    "        else:\n",
    "            attn_output, _ = self.attn(self.norm1(x), self.norm1(x), self.norm1(x))\n",
    "        \n",
    "        x = x + attn_output\n",
    "        \n",
    "        # MLP with residual connection\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "# Define the Audio Transformer encoder\n",
    "class AudioTransformer(nn.Module):\n",
    "    def __init__(self, num_tokens=NUM_TOKENS, embed_dim=LATENT_DIM, depth=NUM_LAYERS, \n",
    "                 num_heads=NUM_HEADS, mlp_ratio=MLP_RATIO, drop_rate=0.1, num_classes=0):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        \n",
    "        # Feature extractor (1D CNN)\n",
    "        self.feature_extractor = CNNFeatureExtractor(embed_dim=embed_dim)\n",
    "        \n",
    "        # Position embeddings\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_tokens + 1, embed_dim))\n",
    "        \n",
    "        # Class token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        \n",
    "        # Dropout\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlock(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, \n",
    "                drop=drop_rate, attn_drop=drop_rate)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # Layer norm\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Classifier head\n",
    "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "    \n",
    "    def forward_features(self, x, mask=None):\n",
    "        # Extract CNN features: (B, C, L) -> (B, L, D)\n",
    "        x = self.feature_extractor(x)\n",
    "        \n",
    "        # Add class token\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        \n",
    "        # Add position embedding (with size check)\n",
    "        # Ensure pos_embed size matches sequence length\n",
    "        if x.size(1) > self.pos_embed.size(1):\n",
    "            # If sequence is longer than position embeddings, truncate sequence\n",
    "            x = x[:, :self.pos_embed.size(1), :]\n",
    "        elif x.size(1) < self.pos_embed.size(1):\n",
    "            # If sequence is shorter than position embeddings, truncate position embeddings\n",
    "            pos_embed = self.pos_embed[:, :x.size(1), :]\n",
    "            x = x + pos_embed\n",
    "        else:\n",
    "            # If sizes match, add directly\n",
    "            x = x + self.pos_embed[:, :x.size(1)]\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        \n",
    "        # Apply final norm\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Forward features\n",
    "        x = self.forward_features(x, mask)\n",
    "        \n",
    "        # Use class token for classification\n",
    "        if self.num_classes > 0:\n",
    "            x = self.head(x[:, 0])\n",
    "        return x\n",
    "\n",
    "# Define the Transformer decoder block\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., drop=0., attn_drop=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.self_attn = nn.MultiheadAttention(dim, num_heads, dropout=attn_drop, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.cross_attn = nn.MultiheadAttention(dim, num_heads, dropout=attn_drop, batch_first=True)\n",
    "        self.norm3 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(in_features=dim, hidden_features=int(dim * mlp_ratio), drop=drop)\n",
    "        \n",
    "    def forward(self, x, context):\n",
    "        # Self-attention\n",
    "        self_attn_output, _ = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x))\n",
    "        x = x + self_attn_output\n",
    "        \n",
    "        # Cross-attention\n",
    "        cross_attn_output, _ = self.cross_attn(self.norm2(x), self.norm2(context), self.norm2(context))\n",
    "        x = x + cross_attn_output\n",
    "        \n",
    "        # MLP\n",
    "        x = x + self.mlp(self.norm3(x))\n",
    "        return x\n",
    "\n",
    "# Define the Transformer decoder\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, embed_dim=LATENT_DIM, depth=NUM_DECODER_LAYERS, \n",
    "                 num_heads=NUM_HEADS, mlp_ratio=MLP_RATIO, drop_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerDecoderBlock(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, \n",
    "                drop=drop_rate, attn_drop=drop_rate)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # Layer norm\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "    \n",
    "    def forward(self, x, context):\n",
    "        # Apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x, context)\n",
    "        \n",
    "        # Apply final norm\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "# Define the AJEPA model for 1D audio\n",
    "class AJEPA1D(nn.Module):\n",
    "    def __init__(self, num_tokens=NUM_TOKENS, embed_dim=LATENT_DIM, \n",
    "                 encoder_depth=NUM_LAYERS, decoder_depth=NUM_DECODER_LAYERS, \n",
    "                 num_heads=NUM_HEADS, mlp_ratio=MLP_RATIO, drop_rate=0.1,\n",
    "                 num_classes=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Context encoder\n",
    "        self.context_encoder = AudioTransformer(\n",
    "            num_tokens=num_tokens, embed_dim=embed_dim, depth=encoder_depth, \n",
    "            num_heads=num_heads, mlp_ratio=mlp_ratio, drop_rate=drop_rate, num_classes=0)\n",
    "        \n",
    "        # Target encoder (updated via EMA)\n",
    "        self.target_encoder = AudioTransformer(\n",
    "            num_tokens=num_tokens, embed_dim=embed_dim, depth=encoder_depth, \n",
    "            num_heads=num_heads, mlp_ratio=mlp_ratio, drop_rate=drop_rate, num_classes=0)\n",
    "        \n",
    "        # Predictor (decoder)\n",
    "        self.predictor = TransformerDecoder(\n",
    "            embed_dim=embed_dim, depth=decoder_depth, \n",
    "            num_heads=num_heads, mlp_ratio=mlp_ratio, drop_rate=drop_rate)\n",
    "        \n",
    "        # Classifier head (for fine-tuning)\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        \n",
    "        # Initialize target encoder with context encoder weights\n",
    "        self._init_target_encoder()\n",
    "        \n",
    "    def _init_target_encoder(self):\n",
    "        # Initialize target encoder with context encoder weights\n",
    "        for param_target, param_context in zip(self.target_encoder.parameters(), self.context_encoder.parameters()):\n",
    "            param_target.data.copy_(param_context.data)\n",
    "            param_target.requires_grad = False\n",
    "    \n",
    "    def update_target_encoder(self, momentum=0.996):\n",
    "        # Update target encoder via EMA\n",
    "        for param_target, param_context in zip(self.target_encoder.parameters(), self.context_encoder.parameters()):\n",
    "            param_target.data = momentum * param_target.data + (1 - momentum) * param_context.data\n",
    "    \n",
    "    def forward_pretrain(self, x, context_masks, target_masks):\n",
    "        # Forward pass with context encoder\n",
    "        context_features = self.context_encoder.forward_features(x, context_masks)\n",
    "        \n",
    "        # Forward pass with target encoder\n",
    "        with torch.no_grad():\n",
    "            target_features = self.target_encoder.forward_features(x, target_masks)\n",
    "        \n",
    "        # Forward pass with predictor\n",
    "        pred_features = self.predictor(context_features, target_features)\n",
    "        \n",
    "        return pred_features, target_features\n",
    "    \n",
    "    def forward_finetune(self, x, mask=None):\n",
    "        # Forward pass with context encoder\n",
    "        features = self.context_encoder.forward_features(x, mask)\n",
    "        \n",
    "        # Extract [CLS] token for classification\n",
    "        cls_token = features[:, 0]\n",
    "        \n",
    "        # Forward pass with classifier\n",
    "        logits = self.classifier(cls_token)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Define mask generator functions for 1D audio signal\n",
    "def generate_block_mask(seq_length, num_blocks=1, min_block_size=10, max_block_size=50):\n",
    "    \"\"\"Generate random block masks for 1D signal\"\"\"\n",
    "    mask = torch.zeros(seq_length, dtype=torch.bool, device=device)\n",
    "    \n",
    "    for _ in range(num_blocks):\n",
    "        # Sample block size\n",
    "        block_size = torch.randint(min_block_size, max_block_size + 1, (1,)).item()\n",
    "        \n",
    "        # Sample start position\n",
    "        start = torch.randint(0, seq_length - block_size + 1, (1,)).item()\n",
    "        \n",
    "        # Set mask\n",
    "        mask[start:start + block_size] = True\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def generate_sparse_mask(seq_length, mask_ratio=0.2):\n",
    "    \"\"\"Generate sparse random masks for 1D signal\"\"\"\n",
    "    num_masked = int(seq_length * mask_ratio)\n",
    "    mask_indices = torch.randperm(seq_length, device=device)[:num_masked]\n",
    "    \n",
    "    mask = torch.zeros(seq_length, dtype=torch.bool, device=device)\n",
    "    mask[mask_indices] = True\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def generate_contextual_mask(signal, seq_length, mask_ratio=0.5, energy_threshold=0.7):\n",
    "    \"\"\"Generate contextual masks based on signal energy\"\"\"\n",
    "    # Compute energy\n",
    "    if len(signal.shape) > 2:  # Handle batch dimension\n",
    "        # Average across batch\n",
    "        energy = torch.mean(torch.abs(signal), dim=0).squeeze()\n",
    "    else:\n",
    "        energy = torch.abs(signal).squeeze()\n",
    "    \n",
    "    # Normalize energy\n",
    "    energy = energy / (energy.max() + 1e-6)\n",
    "    \n",
    "    # Downsample energy to match seq_length if needed\n",
    "    if energy.shape[0] > seq_length:\n",
    "        # Average pooling to downsample\n",
    "        energy = F.avg_pool1d(energy.unsqueeze(0).unsqueeze(0), \n",
    "                              kernel_size=energy.shape[0] // seq_length,\n",
    "                              stride=energy.shape[0] // seq_length).squeeze()\n",
    "    \n",
    "    # Create mask based on energy\n",
    "    high_energy_regions = (energy > energy_threshold)\n",
    "    \n",
    "    # Ensure masking ratio is met\n",
    "    if high_energy_regions.sum() < seq_length * mask_ratio:\n",
    "        # Add more masks randomly if needed\n",
    "        remaining = int(seq_length * mask_ratio) - high_energy_regions.sum().item()\n",
    "        low_energy_indices = torch.where(~high_energy_regions)[0]\n",
    "        if len(low_energy_indices) > 0:\n",
    "            additional_indices = low_energy_indices[torch.randperm(len(low_energy_indices))[:remaining]]\n",
    "            high_energy_regions[additional_indices] = True\n",
    "    elif high_energy_regions.sum() > seq_length * mask_ratio:\n",
    "        # Remove some masks if needed\n",
    "        excess = high_energy_regions.sum().item() - int(seq_length * mask_ratio)\n",
    "        high_energy_indices = torch.where(high_energy_regions)[0]\n",
    "        remove_indices = high_energy_indices[torch.randperm(len(high_energy_indices))[:excess]]\n",
    "        high_energy_regions[remove_indices] = False\n",
    "    \n",
    "    return high_energy_regions\n",
    "\n",
    "def generate_curriculum_masks(batch_size, seq_length, inputs=None, curriculum_step=0.0):\n",
    "    \"\"\"\n",
    "    Generate masks for curriculum learning.\n",
    "    curriculum_step: 0.0 -> 1.0 from simple block to contextual masking.\n",
    "    \"\"\"\n",
    "    context_masks = []\n",
    "    target_masks = []\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        # Determine masking stage based on curriculum_step\n",
    "        if curriculum_step < 0.33:\n",
    "            # Stage 1: Simple block masking (15-20% mask ratio)\n",
    "            mask_ratio = 0.15 + 0.05 * curriculum_step / 0.33\n",
    "            num_blocks = max(1, int(seq_length * mask_ratio / 30))\n",
    "            target_mask = generate_block_mask(seq_length, num_blocks=num_blocks, \n",
    "                                            min_block_size=20, max_block_size=40)\n",
    "            \n",
    "        elif curriculum_step < 0.67:\n",
    "            # Stage 2: Mixed masking (30-40% mask ratio)\n",
    "            norm_step = (curriculum_step - 0.33) / 0.34\n",
    "            mask_ratio = 0.3 + 0.1 * norm_step\n",
    "            \n",
    "            block_ratio = 0.7 - 0.4 * norm_step\n",
    "            sparse_ratio = 1.0 - block_ratio\n",
    "            \n",
    "            block_mask = generate_block_mask(seq_length, \n",
    "                                            num_blocks=int(seq_length * mask_ratio * block_ratio / 30),\n",
    "                                            min_block_size=15, max_block_size=35)\n",
    "            sparse_mask = generate_sparse_mask(seq_length, mask_ratio=mask_ratio * sparse_ratio)\n",
    "            \n",
    "            # Move both masks to the same device before combining\n",
    "            block_mask = block_mask.to(device)\n",
    "            sparse_mask = sparse_mask.to(device)\n",
    "            target_mask = block_mask | sparse_mask\n",
    "            \n",
    "        else:\n",
    "            # Stage 3: Contextual-aware masking (50-60% mask ratio)\n",
    "            norm_step = (curriculum_step - 0.67) / 0.33\n",
    "            mask_ratio = 0.5 + 0.1 * norm_step\n",
    "            \n",
    "            if inputs is not None:\n",
    "                # Use signal for contextual masking\n",
    "                target_mask = generate_contextual_mask(inputs[b:b+1], seq_length, \n",
    "                                                    mask_ratio=mask_ratio,\n",
    "                                                    energy_threshold=0.5)\n",
    "            else:\n",
    "                # Fallback to mixed masking if inputs not provided\n",
    "                block_mask = generate_block_mask(seq_length, \n",
    "                                                num_blocks=int(seq_length * mask_ratio * 0.3 / 30),\n",
    "                                                min_block_size=10, max_block_size=30)\n",
    "                sparse_mask = generate_sparse_mask(seq_length, mask_ratio=mask_ratio * 0.7)\n",
    "                \n",
    "                # Move both masks to the same device before combining\n",
    "                block_mask = block_mask.to(device)\n",
    "                sparse_mask = sparse_mask.to(device)\n",
    "                target_mask = block_mask | sparse_mask\n",
    "        \n",
    "        # Move target_mask to device if not already\n",
    "        target_mask = target_mask.to(device)\n",
    "        \n",
    "        # Create complement mask for context (95% visible)\n",
    "        context_visible_ratio = 0.95\n",
    "        context_mask = generate_sparse_mask(seq_length, mask_ratio=(1.0 - context_visible_ratio))\n",
    "        context_mask = context_mask.to(device)  # Move to the same device\n",
    "        \n",
    "        # Ensure context and target masks don't completely overlap\n",
    "        overlap = context_mask & target_mask  # Now both are on the same device\n",
    "        if overlap.sum() > 0.8 * target_mask.sum():\n",
    "            # Too much overlap, regenerate context mask\n",
    "            remaining_indices = torch.where(~target_mask)[0]\n",
    "            if len(remaining_indices) > 0:\n",
    "                num_context_masked = int(seq_length * (1.0 - context_visible_ratio))\n",
    "                num_context_masked = min(num_context_masked, len(remaining_indices))\n",
    "                \n",
    "                context_indices = remaining_indices[torch.randperm(len(remaining_indices), device=device)[:num_context_masked]]\n",
    "                context_mask = torch.zeros(seq_length, dtype=torch.bool, device=device)\n",
    "                context_mask[context_indices] = True\n",
    "        \n",
    "        # Add padding for the [CLS] token (already on the correct device)\n",
    "        context_attn_mask = torch.cat([torch.zeros((1), dtype=torch.bool, device=device), \n",
    "                                      context_mask], dim=0)\n",
    "        target_attn_mask = torch.cat([torch.zeros((1), dtype=torch.bool, device=device), \n",
    "                                     target_mask], dim=0)\n",
    "        \n",
    "        context_masks.append(context_attn_mask)\n",
    "        target_masks.append(target_attn_mask)\n",
    "    \n",
    "    # Stack masks (already on the correct device)\n",
    "    context_masks = torch.stack(context_masks)\n",
    "    target_masks = torch.stack(target_masks)\n",
    "    \n",
    "    return context_masks, target_masks\n",
    "\n",
    "def generate_regularized_mask(batch_size, seq_length, mask_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Generate regularized mask for fine-tuning.\n",
    "    \"\"\"\n",
    "    # Create masks\n",
    "    masks = []\n",
    "    \n",
    "    for _ in range(batch_size):\n",
    "        # Randomly select tokens to mask\n",
    "        num_masked = int(seq_length * mask_ratio)\n",
    "        mask_indices = torch.randperm(seq_length)[:num_masked]\n",
    "        \n",
    "        # Create mask\n",
    "        mask = torch.zeros(seq_length, dtype=torch.bool)\n",
    "        mask[mask_indices] = True\n",
    "        \n",
    "        # Add padding for the [CLS] token\n",
    "        mask = torch.cat([torch.zeros((1), dtype=torch.bool), mask], dim=0)\n",
    "        masks.append(mask)\n",
    "    \n",
    "    # Stack masks\n",
    "    masks = torch.stack(masks).to(device)\n",
    "    \n",
    "    return masks\n",
    "\n",
    "# Define the pretraining function\n",
    "def pretrain(model, train_loader, num_epochs, optimizer, scheduler=None):\n",
    "    model.train()\n",
    "    \n",
    "    # Loss function (cosine similarity loss)\n",
    "    cos_sim = nn.CosineSimilarity(dim=-1)\n",
    "    \n",
    "    # Create gradient scaler for mixed precision\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Curriculum step (0 -> 1 over epochs)\n",
    "        curriculum_step = min(1.0, epoch / (num_epochs * 0.8))\n",
    "        \n",
    "        # Progress bar\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for i, (inputs, _) in enumerate(progress_bar):\n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            # Generate masks\n",
    "            context_masks, target_masks = generate_curriculum_masks(\n",
    "                inputs.shape[0], NUM_TOKENS, inputs, curriculum_step=curriculum_step)\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with autocast():\n",
    "                pred_features, target_features = model.forward_pretrain(inputs, context_masks, target_masks)\n",
    "                \n",
    "                # Calculate loss (negative cosine similarity)\n",
    "                loss = -cos_sim(pred_features[:, 1:], target_features[:, 1:]).mean()\n",
    "            \n",
    "            # Backward pass and optimize with scaler\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Update target encoder\n",
    "            model.update_target_encoder()\n",
    "            \n",
    "            # Update running loss\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "            \n",
    "        # Print epoch loss\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "        \n",
    "        # Step scheduler\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Progress bar\n",
    "    progress_bar = tqdm(test_loader, desc='Evaluating')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            # Generate regularized masks\n",
    "            masks = generate_regularized_mask(inputs.shape[0], NUM_TOKENS, mask_ratio=MASK_RATIO_FINETUNE)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model.forward_finetune(inputs, masks)\n",
    "            \n",
    "            # Store predictions\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    return accuracy, all_preds, all_labels\n",
    "\n",
    "# Define the fine-tuning function\n",
    "def finetune(model, train_loader, val_loader, num_epochs, optimizer, scheduler=None):\n",
    "    best_val_acc = 0.0\n",
    "    best_model_weights = None\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # To store metrics for plotting\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(progress_bar):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            masks = generate_regularized_mask(inputs.shape[0], NUM_TOKENS, mask_ratio=MASK_RATIO_FINETUNE)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model.forward_finetune(inputs, masks)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = accuracy_score(all_labels, all_preds)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        all_val_preds = []\n",
    "        all_val_labels = []\n",
    "\n",
    "        val_progress_bar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, labels) in enumerate(val_progress_bar):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                masks = generate_regularized_mask(inputs.shape[0], NUM_TOKENS, mask_ratio=MASK_RATIO_FINETUNE)\n",
    "\n",
    "                outputs = model.forward_finetune(inputs, masks)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_running_loss += loss.item()\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_val_preds.extend(preds.cpu().numpy())\n",
    "                all_val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "                val_progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        val_loss = val_running_loss / len(val_loader)\n",
    "        val_acc = accuracy_score(all_val_labels, all_val_preds)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_weights = model.state_dict().copy()\n",
    "            print(f'  New best model! Val Acc: {val_acc:.4f}')\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    print(f'Best validation accuracy: {best_val_acc:.4f}')\n",
    "\n",
    "    # Plot Learning Curves\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Epoch vs Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, num_epochs + 1), train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Epoch vs Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Function to visualize masking\n",
    "def visualize_masking(waveform, mask, title=\"Masked Audio\"):\n",
    "    \"\"\"Visualize the original waveform and the masked regions.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot original waveform\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(waveform.squeeze().cpu().numpy())\n",
    "    plt.title(\"Original Waveform\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    \n",
    "    # Create mask visualization\n",
    "    plt.subplot(2, 1, 2)\n",
    "    \n",
    "    # Upsample mask to match waveform length if needed\n",
    "    if len(mask) != waveform.shape[1]:\n",
    "        binary_mask = torch.zeros(waveform.shape[1])\n",
    "        mask_len = len(mask) - 1  # Account for CLS token\n",
    "        step = waveform.shape[1] // mask_len\n",
    "        \n",
    "        for i in range(mask_len):\n",
    "            if mask[i+1]:  # Skip CLS token\n",
    "                binary_mask[i*step:(i+1)*step] = 1\n",
    "    else:\n",
    "        binary_mask = mask\n",
    "    \n",
    "    masked_waveform = waveform.clone().squeeze().cpu().numpy()\n",
    "    mask_indices = binary_mask.bool().cpu().numpy()\n",
    "    masked_waveform[mask_indices] = np.nan  # Set masked regions to NaN\n",
    "    \n",
    "    plt.plot(masked_waveform)\n",
    "    plt.title(f\"{title} (masked regions removed)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.xlabel(\"Sample\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_masking(waveform, mask, title):\n",
    "    \"\"\"Visualize the original waveform and the masked version.\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Ensure dimensions match before masking\n",
    "    waveform_np = waveform.clone().squeeze().cpu().numpy()\n",
    "    mask_np = mask.squeeze().cpu().numpy()\n",
    "    \n",
    "    # Make sure mask is the same length as waveform\n",
    "    if len(mask_np) != len(waveform_np):\n",
    "        # Resize mask to match waveform length\n",
    "        if len(mask_np) == 1:  # Special case: singleton mask\n",
    "            mask_np = np.repeat(mask_np, len(waveform_np))\n",
    "        else:\n",
    "            # Resample mask to match waveform length\n",
    "            indices = np.round(np.linspace(0, len(mask_np) - 1, len(waveform_np))).astype(int)\n",
    "            mask_np = mask_np[indices]\n",
    "    \n",
    "    # Create masked waveform\n",
    "    masked_waveform = waveform_np.copy()\n",
    "    masked_waveform[mask_np > 0.5] = np.nan  # Set masked regions to NaN\n",
    "    \n",
    "    plt.plot(masked_waveform)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Sample\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    print(\"Starting A-JEPA with 1D CNN implementation for Environment Sound Classification\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "        print(\"CUDA cache cleared.\")\n",
    "    else:\n",
    "        print(\"CUDA is not available.\")\n",
    "    \n",
    "    # Load full dataset and update class list\n",
    "    global ENV_SOUND_CLASSES, NUM_CLASSES\n",
    "    \n",
    "    # Check if the dataset path exists\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        print(f\"Error: Dataset path {DATA_PATH} does not exist.\")\n",
    "        print(\"Available paths:\")\n",
    "        for path in os.listdir('/kaggle/input'):\n",
    "            print(f\" - /kaggle/input/{path}\")\n",
    "        return\n",
    "    \n",
    "    # Get the actual class names from the directory\n",
    "    ENV_SOUND_CLASSES = sorted([d for d in os.listdir(DATA_PATH) if os.path.isdir(os.path.join(DATA_PATH, d))])\n",
    "    NUM_CLASSES = len(ENV_SOUND_CLASSES)\n",
    "    \n",
    "    print(f\"Found {NUM_CLASSES} classes: {ENV_SOUND_CLASSES}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    full_dataset = EnvironmentSoundDataset(DATA_PATH)\n",
    "    print(f'Dataset loaded, size: {len(full_dataset)}')\n",
    "    \n",
    "    # Train/Validation split\n",
    "    indices = list(range(len(full_dataset)))\n",
    "    train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42, stratify=full_dataset.labels)\n",
    "\n",
    "    train_dataset = Subset(full_dataset, train_indices)\n",
    "    val_dataset = Subset(full_dataset, val_indices)\n",
    "\n",
    "    # Create data loaders (use num_workers=0 if having issues with parallelism in Kaggle)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Create model\n",
    "    model = AJEPA1D(\n",
    "        num_tokens=NUM_TOKENS,\n",
    "        embed_dim=LATENT_DIM,\n",
    "        encoder_depth=NUM_LAYERS,\n",
    "        decoder_depth=NUM_DECODER_LAYERS,\n",
    "        num_heads=NUM_HEADS,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        mlp_ratio=MLP_RATIO,\n",
    "        drop_rate=0.1\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"Model created with parameters:\")\n",
    "    print(f\"- Tokens: {NUM_TOKENS}\")\n",
    "    print(f\"- Sample rate: {SAMPLE_RATE}\")\n",
    "    print(f\"- CNN kernel size: {CNN_KERNEL_SIZE} ({CNN_KERNEL_SIZE/SAMPLE_RATE*1000:.1f}ms)\")\n",
    "    print(f\"- CNN stride: {CNN_STRIDE} ({CNN_STRIDE/SAMPLE_RATE*1000:.1f}ms)\")\n",
    "    print(f\"- Latent dimension: {LATENT_DIM}\")\n",
    "    print(f\"- Encoder layers: {NUM_LAYERS}\")\n",
    "    print(f\"- Decoder layers: {NUM_DECODER_LAYERS}\")\n",
    "    print(f\"- Attention heads: {NUM_HEADS}\")\n",
    "    print(f\"- Classes: {NUM_CLASSES} ({ENV_SOUND_CLASSES})\")\n",
    "    \n",
    "    # --- Pretraining ---\n",
    "    print(\"\\n=== Starting Pretraining ===\")\n",
    "    \n",
    "    # Create optimizer for pretraining\n",
    "    pretrain_optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE_PRETRAIN,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    # Create learning rate scheduler\n",
    "    pretrain_scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        pretrain_optimizer,\n",
    "        T_max=NUM_EPOCHS_PRETRAIN\n",
    "    )\n",
    "    \n",
    "    # Pretrain model\n",
    "    model = pretrain(\n",
    "        model,\n",
    "        train_loader,\n",
    "        NUM_EPOCHS_PRETRAIN,\n",
    "        pretrain_optimizer,\n",
    "        pretrain_scheduler\n",
    "    )\n",
    "    \n",
    "    # Save pretrained model checkpoint\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': pretrain_optimizer.state_dict(),\n",
    "    }, 'ajepa_1d_pretrained.pth')\n",
    "    \n",
    "    print(\"Pretraining completed and checkpoint saved.\")\n",
    "    \n",
    "    # --- Fine-tuning ---\n",
    "    print(\"\\n=== Starting Fine-tuning ===\")\n",
    "    \n",
    "    # Create optimizer for fine-tuning\n",
    "    finetune_optimizer = optim.AdamW(\n",
    "        [\n",
    "            {'params': model.context_encoder.parameters()},\n",
    "            {'params': model.classifier.parameters(), 'lr': LEARNING_RATE_FINETUNE * 10}\n",
    "        ],\n",
    "        lr=LEARNING_RATE_FINETUNE,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    # Create learning rate scheduler\n",
    "    finetune_scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        finetune_optimizer,\n",
    "        T_max=NUM_EPOCHS_FINETUNE\n",
    "    )\n",
    "    \n",
    "    # Fine-tune model\n",
    "    model = finetune(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        NUM_EPOCHS_FINETUNE,\n",
    "        finetune_optimizer,\n",
    "        finetune_scheduler\n",
    "    )\n",
    "    \n",
    "    # Save fine-tuned model checkpoint\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': finetune_optimizer.state_dict(),\n",
    "    }, 'ajepa_1d_finetuned.pth')\n",
    "    \n",
    "    print(\"Fine-tuning completed and checkpoint saved.\")\n",
    "    \n",
    "    # --- Evaluation ---\n",
    "    print(\"\\n=== Evaluating Model ===\")\n",
    "    \n",
    "    # Evaluate model\n",
    "    test_accuracy, test_preds, test_labels = evaluate(model, val_loader)\n",
    "    \n",
    "    # Visualize confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import seaborn as sns\n",
    "    \n",
    "    cm = confusion_matrix(test_labels, test_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=ENV_SOUND_CLASSES, yticklabels=ENV_SOUND_CLASSES)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Final Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"Confusion matrix saved as 'confusion_matrix.png'\")\n",
    "        \n",
    "    # Main visualization code\n",
    "    print(\"\\n=== Visualizing Masking Strategies ===\")\n",
    "        \n",
    "    # Get a sample from the validation set\n",
    "    sample_inputs, _ = next(iter(val_loader))\n",
    "    sample_input = sample_inputs[0:1]  # Take first sample\n",
    "        \n",
    "    # Generate masks for different curriculum stages\n",
    "    for curriculum_step, stage_name in [(0.1, \"Stage 1 (Easy)\"), \n",
    "                                      (0.5, \"Stage 2 (Medium)\"), \n",
    "                                      (0.9, \"Stage 3 (Hard)\")]:\n",
    "        # Generate masks\n",
    "        __, target_masks = generate_curriculum_masks(\n",
    "            1, NUM_TOKENS, sample_input, curriculum_step=curriculum_step)\n",
    "        \n",
    "        # Pass directly to the updated visualize_masking function\n",
    "        # which now handles size mismatches internally\n",
    "        visualize_masking(sample_input, target_masks[0], title=f\"Masking: {stage_name}\")\n",
    "        \n",
    "    print(\"Masking visualization completed.\")\n",
    "    \n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93411f4b",
   "metadata": {
    "papermill": {
     "duration": 0.001783,
     "end_time": "2025-11-24T09:12:25.325407",
     "exception": false,
     "start_time": "2025-11-24T09:12:25.323624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7301102,
     "sourceId": 11636372,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 22.002987,
   "end_time": "2025-11-24T09:12:28.892314",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-24T09:12:06.889327",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
